{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import gensim.downloader as api\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import torch\n",
    "import time\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTILS FUNCTIONS :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_from_dataframe(df, column_name='Tweet', model_name='bert-base-uncased', batch_size=1024):\n",
    "    \"\"\"\n",
    "    Calcule les embeddings pour une colonne de tweets dans un DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame contenant les tweets.\n",
    "        column_name (str): Nom de la colonne contenant les tweets.\n",
    "        model_name (str): Nom du modèle Transformer.\n",
    "        batch_size (int): Taille des lots pour le traitement en batch.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Tableau 2D contenant les embeddings des tweets.\n",
    "    \"\"\"\n",
    "    # Charger le tokenizer et le modèle\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    model.eval()  # Mode évaluation\n",
    "\n",
    "    # Si un GPU est disponible, utiliser le GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Préparer les données\n",
    "    tweets = df[column_name].tolist()\n",
    "    embeddings = []\n",
    "\n",
    "    # Traiter les tweets par lots\n",
    "    for i in tqdm(range(0, len(tweets), batch_size), desc=\"Processing tweets\"):\n",
    "        batch_tweets = tweets[i:i + batch_size]\n",
    "\n",
    "        # Tokenisation et transfert sur le GPU\n",
    "        inputs = tokenizer(batch_tweets, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "        # Passage au modèle\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Extraire les vecteurs CLS\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :].detach().cpu().numpy()\n",
    "        embeddings.append(cls_embeddings)\n",
    "\n",
    "    # Combiner tous les embeddings\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformer_embedding(tweet, model_name='bert-base-uncased'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    # Tokenisation et passage au modèle\n",
    "    inputs = tokenizer(tweet, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # Utiliser le vecteur CLS (représente le tweet entier)\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :].detach().numpy()\n",
    "    return cls_embedding.squeeze()\n",
    "\n",
    "# Function to compute the average word vector for a tweet\n",
    "# def get_avg_embedding(tweet, model, vector_size=200):\n",
    "#     words = tweet.split()  # Tokenize by whitespace\n",
    "#     word_vectors = [model[word] for word in words if word in model]\n",
    "#     if not word_vectors:  # If no words in the tweet are in the vocabulary, return a zero vector\n",
    "#         return np.zeros(vector_size)\n",
    "#     return np.mean(word_vectors, axis=0)\n",
    "\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "sym_spell.load_dictionary(\"en-80k.txt\", term_index=0, count_index=1)\n",
    "# Correct word\n",
    "def correct_text(text):\n",
    "    sug = sym_spell.lookup(text, Verbosity.CLOSEST, max_edit_distance=2)\n",
    "    if sug:\n",
    "        return sug[0].term\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "\n",
    "# Basic preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Tokenization\n",
    "    words = text.split()\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [correct_text(word) for word in words if word not in stop_words]\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "# Calculate sentiment rate of a text\n",
    "def get_sentiment_rate(text):\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    return np.abs(scores['compound'])\n",
    "\n",
    "\n",
    "football_words = [\"full time\", \"goal\", \"half time\", \"kick off\", \"owngoal\", \"penalty\", \"match\", \"red card\", \"yellow card\"]\n",
    "# Calculate the number of football words in a tweet\n",
    "def count_football_words(text):\n",
    "    return sum(word in text for word in football_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCCESS PART 0 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PREPROCESS PART 0...\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "\n",
    "os.makedirs(\"tmp/\", exist_ok = True)\n",
    "\n",
    "\n",
    "# Download some NLP models for processing, optional\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load GloVe model with Gensim's API\n",
    "# embeddings_model = api.load(\"glove-twitter-200\")  # 200-dimensional GloVe embeddings\n",
    "\n",
    "\n",
    "print(\"PREPROCESS PART 0 : OK\")\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESS PART 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PREPROCESS PART 1...\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "\n",
    "go = True\n",
    "\n",
    "\n",
    "if go or not os.path.isfile(\"tmp/processing1.csv\"):\n",
    "    # Read all training files and concatenate them into one dataframe\n",
    "    li = []\n",
    "    for filename in os.listdir(\"train_tweets\"):\n",
    "        df = pd.read_csv(\"train_tweets/\" + filename)\n",
    "        li.append(df)\n",
    "    df = pd.concat(li, ignore_index=True)\n",
    "\n",
    "    # Apply preprocessing to each tweet\n",
    "    df['Tweet'] = df['Tweet'].apply(preprocess_text)\n",
    "\n",
    "    df.to_csv(\"tmp/processing1.csv\", index=False, encoding=\"utf-8\")\n",
    "else:\n",
    "    df = pd.read_csv(\"tmp/processing1.csv\")\n",
    "\n",
    "\n",
    "print(\"PREPROCESS PART 1 : OK\")\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESS PART 2 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PREPROCESS PART 2...\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "\n",
    "go = True\n",
    "\n",
    "\n",
    "if go or not os.path.isfile(\"tmp/X.npy\") or not os.path.isfile(\"tmp/y.npy\"):\n",
    "\n",
    "    limit = int(len(df) / 2)\n",
    "    \n",
    "    if go or not os.path.isfile(\"tmp/df1.npy\"):\n",
    "        # Calcul la 1ère partie de l'embeddings\n",
    "        tweet_vectors1 = get_embeddings_from_dataframe(df[:limit], column_name='Tweet', batch_size=4096)\n",
    "        tweet_df1 = pd.DataFrame(tweet_vectors1)\n",
    "        np.save(\"tmp/df1.npy\", tweet_df1)\n",
    "    else:\n",
    "        tweet_df1 = np.load(\"tmp/df1.npy\")\n",
    "\n",
    "    print(\"df1 : OK\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    if go or not os.path.isfile(\"tmp/df1.npy\"):\n",
    "        # Calcul la 2ème partie de l'embeddings\n",
    "        tweet_vectors2 = get_embeddings_from_dataframe(df[limit:], column_name='Tweet', batch_size=4096)\n",
    "        tweet_df2 = pd.DataFrame(tweet_vectors2)\n",
    "        np.save(\"tmp/df2.npy\", tweet_df2)\n",
    "    else:\n",
    "        tweet_df2 = np.load(\"tmp/df2.npy\")\n",
    "\n",
    "    print(\"df2 : OK\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # Join les df\n",
    "    period_features = pd.concat([tweet_df1, tweet_df2], axis=1)\n",
    "\n",
    "    print(\"period_features : OK\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "    # Ajouter une colonne contenant le nombre de tweets par PeriodID\n",
    "    period_features['TweetCount'] = period_features.groupby(['MatchID', 'PeriodID', 'ID'])['Tweet'].transform('size').fillna(0)\n",
    "    period_features['TweetCount'] = period_features['TweetCount'] / period_features['TweetCount'].max()\n",
    "    \n",
    "    # Ajouter une colonne contenant le nombre de mots liés au foot par tweet\n",
    "    period_features['FootballWordCount'] = period_features['Tweet'].apply(count_football_words).fillna(0)\n",
    "    period_features['FootballWordCount'] = period_features['FootballWordCount'] / period_features['FootballWordCount'].max()\n",
    "    \n",
    "    # Ajouter une colonne contenant le score de sentiment\n",
    "    period_features['Sentiment'] = period_features['Tweet'].apply(get_sentiment_rate).fillna(0)\n",
    "\n",
    "    print(\"add colonnes : OK\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    \n",
    "    # Drop the columns that are not useful anymore\n",
    "    period_features = period_features.drop(columns=['Timestamp', 'Tweet'])\n",
    "    \n",
    "    # Group the tweets into their corresponding periods. This way we generate an average embedding vector for each period\n",
    "    period_features = period_features.groupby(['MatchID', 'PeriodID', 'ID']).mean().reset_index()\n",
    "    \n",
    "    # We drop the non-numerical features and keep the embeddings values for each period\n",
    "    X = period_features.drop(columns=['EventType', 'MatchID', 'PeriodID', 'ID']).values\n",
    "    # We extract the labels of our training samples\n",
    "    y = period_features['EventType'].values\n",
    "    \n",
    "    np.save(\"tmp/X.npy\", X)\n",
    "    np.save(\"tmp/y.npy\", y)\n",
    "\n",
    "    print(\"X-y : OK\")\n",
    "    sys.stdout.flush()\n",
    "else:\n",
    "    X = np.load(\"tmp/X.npy\")\n",
    "    y = np.load(\"tmp/y.npy\")\n",
    "    \n",
    "\n",
    "\n",
    "print(\"PREPROCESS PART 2 : OK\")\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Kaggle submission :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"KAGGLE...\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "\n",
    "# This time we train our classifier on the full dataset that it is available to us.\n",
    "\n",
    "clf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "clf.fit(X, y)\n",
    "predictions = []\n",
    "\n",
    "\n",
    "# We read each file separately, we preprocess the tweets and then use the classifier to predict the labels.\n",
    "# Finally, we concatenate all predictions into a list that will eventually be concatenated and exported\n",
    "# to be submitted on Kaggle.\n",
    "for fname in os.listdir(\"eval_tweets\"):\n",
    "    val_df = pd.read_csv(\"eval_tweets/\" + fname)\n",
    "    val_df['Tweet'] = val_df['Tweet'].apply(preprocess_text)\n",
    "\n",
    "    ###\n",
    "    limit = int(len(val_df) / 2)\n",
    "    \n",
    "    tweet_vectors1 = get_embeddings_from_dataframe(val_df[:limit], column_name='Tweet', batch_size=4096)\n",
    "    tweet_df1 = pd.DataFrame(tweet_vectors1)\n",
    "\n",
    "    tweet_vectors2 = get_embeddings_from_dataframe(val_df[limit:], column_name='Tweet', batch_size=4096)\n",
    "    tweet_df2 = pd.DataFrame(tweet_vectors2)\n",
    "\n",
    "    period_features = pd.concat([tweet_df1, tweet_df2], axis=1)\n",
    "    ###\n",
    "\n",
    "    ###\n",
    "    period_features['TweetCount'] = period_features.groupby(['MatchID', 'PeriodID', 'ID'])['Tweet'].transform('size').fillna(0)\n",
    "    period_features['TweetCount'] = period_features['TweetCount'] / period_features['TweetCount'].max()\n",
    "\n",
    "    period_features['FootballWordCount'] = period_features['Tweet'].apply(count_football_words).fillna(0)\n",
    "    period_features['FootballWordCount'] = period_features['FootballWordCount'] / period_features['FootballWordCount'].max()\n",
    "\n",
    "    period_features['Sentiment'] = period_features['Tweet'].apply(get_sentiment_rate).fillna(0)\n",
    "    ###\n",
    "\n",
    "    period_features = period_features.drop(columns=['Timestamp', 'Tweet'])\n",
    "    period_features = period_features.groupby(['MatchID', 'PeriodID', 'ID']).mean().reset_index()\n",
    "    X_pred = period_features.drop(columns=['MatchID', 'PeriodID', 'ID']).values\n",
    "\n",
    "    preds = clf.predict(X_pred)\n",
    "    period_features['EventType'] = preds\n",
    "    predictions.append(period_features[['ID', 'EventType']])\n",
    "\n",
    "pred_df = pd.concat(predictions)\n",
    "pred_df.to_csv('predictions_rf_all.csv', index=False)\n",
    "\n",
    "\n",
    "print(\"KAGGLE : OK\")\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"KAGGLE...\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "\n",
    "# This time we train our classifier on the full dataset that it is available to us.\n",
    "\n",
    "clf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "clf.fit(X, y)\n",
    "predictions = []\n",
    "\n",
    "\n",
    "# We read each file separately, we preprocess the tweets and then use the classifier to predict the labels.\n",
    "# Finally, we concatenate all predictions into a list that will eventually be concatenated and exported\n",
    "# to be submitted on Kaggle.\n",
    "for fname in os.listdir(\"eval_tweets\"):\n",
    "    val_df = pd.read_csv(\"eval_tweets/\" + fname)\n",
    "    val_df['Tweet'] = val_df['Tweet'].apply(preprocess_text)\n",
    "\n",
    "    ###\n",
    "    limit = int(len(val_df) / 2)\n",
    "    \n",
    "    tweet_vectors1 = get_embeddings_from_dataframe(val_df[:limit], column_name='Tweet', batch_size=4096)\n",
    "    tweet_df1 = pd.DataFrame(tweet_vectors1)\n",
    "\n",
    "    tweet_vectors2 = get_embeddings_from_dataframe(val_df[limit:], column_name='Tweet', batch_size=4096)\n",
    "    tweet_df2 = pd.DataFrame(tweet_vectors2)\n",
    "\n",
    "    period_features = pd.concat([tweet_df1, tweet_df2], axis=1)\n",
    "    ###\n",
    "\n",
    "    period_features = period_features.drop(columns=['Timestamp', 'Tweet'])\n",
    "    period_features = period_features.groupby(['MatchID', 'PeriodID', 'ID']).mean().reset_index()\n",
    "    X_pred = period_features.drop(columns=['MatchID', 'PeriodID', 'ID']).values\n",
    "\n",
    "    preds = clf.predict(X_pred)\n",
    "    period_features['EventType'] = preds\n",
    "    predictions.append(period_features[['ID', 'EventType']])\n",
    "\n",
    "pred_df = pd.concat(predictions)\n",
    "pred_df.to_csv('predictions_rf_simple.csv', index=False)\n",
    "\n",
    "\n",
    "print(\"KAGGLE : OK\")\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"KAGGLE...\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "\n",
    "# This time we train our classifier on the full dataset that it is available to us.\n",
    "\n",
    "clf = XGBClassifier(random_state=42, n_estimators=100)\n",
    "clf.fit(X, y)\n",
    "predictions = []\n",
    "\n",
    "\n",
    "# We read each file separately, we preprocess the tweets and then use the classifier to predict the labels.\n",
    "# Finally, we concatenate all predictions into a list that will eventually be concatenated and exported\n",
    "# to be submitted on Kaggle.\n",
    "for fname in os.listdir(\"eval_tweets\"):\n",
    "    val_df = pd.read_csv(\"eval_tweets/\" + fname)\n",
    "    val_df['Tweet'] = val_df['Tweet'].apply(preprocess_text)\n",
    "\n",
    "    ###\n",
    "    limit = int(len(val_df) / 2)\n",
    "    \n",
    "    tweet_vectors1 = get_embeddings_from_dataframe(val_df[:limit], column_name='Tweet', batch_size=4096)\n",
    "    tweet_df1 = pd.DataFrame(tweet_vectors1)\n",
    "\n",
    "    tweet_vectors2 = get_embeddings_from_dataframe(val_df[limit:], column_name='Tweet', batch_size=4096)\n",
    "    tweet_df2 = pd.DataFrame(tweet_vectors2)\n",
    "\n",
    "    period_features = pd.concat([tweet_df1, tweet_df2], axis=1)\n",
    "    ###\n",
    "\n",
    "    ###\n",
    "    period_features['TweetCount'] = period_features.groupby(['MatchID', 'PeriodID', 'ID'])['Tweet'].transform('size').fillna(0)\n",
    "    period_features['TweetCount'] = period_features['TweetCount'] / period_features['TweetCount'].max()\n",
    "\n",
    "    period_features['FootballWordCount'] = period_features['Tweet'].apply(count_football_words).fillna(0)\n",
    "    period_features['FootballWordCount'] = period_features['FootballWordCount'] / period_features['FootballWordCount'].max()\n",
    "\n",
    "    period_features['Sentiment'] = period_features['Tweet'].apply(get_sentiment_rate).fillna(0)\n",
    "    ###\n",
    "\n",
    "    period_features = period_features.drop(columns=['Timestamp', 'Tweet'])\n",
    "    period_features = period_features.groupby(['MatchID', 'PeriodID', 'ID']).mean().reset_index()\n",
    "    X_pred = period_features.drop(columns=['MatchID', 'PeriodID', 'ID']).values\n",
    "\n",
    "    preds = clf.predict(X_pred)\n",
    "    period_features['EventType'] = preds\n",
    "    predictions.append(period_features[['ID', 'EventType']])\n",
    "\n",
    "pred_df = pd.concat(predictions)\n",
    "pred_df.to_csv('predictions_rf_all.csv', index=False)\n",
    "\n",
    "\n",
    "print(\"KAGGLE : OK\")\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"KAGGLE...\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "\n",
    "# This time we train our classifier on the full dataset that it is available to us.\n",
    "\n",
    "clf = XGBClassifier(random_state=42, n_estimators=100)\n",
    "clf.fit(X, y)\n",
    "predictions = []\n",
    "\n",
    "\n",
    "# We read each file separately, we preprocess the tweets and then use the classifier to predict the labels.\n",
    "# Finally, we concatenate all predictions into a list that will eventually be concatenated and exported\n",
    "# to be submitted on Kaggle.\n",
    "for fname in os.listdir(\"eval_tweets\"):\n",
    "    val_df = pd.read_csv(\"eval_tweets/\" + fname)\n",
    "    val_df['Tweet'] = val_df['Tweet'].apply(preprocess_text)\n",
    "\n",
    "    ###\n",
    "    limit = int(len(val_df) / 2)\n",
    "    \n",
    "    tweet_vectors1 = get_embeddings_from_dataframe(val_df[:limit], column_name='Tweet', batch_size=4096)\n",
    "    tweet_df1 = pd.DataFrame(tweet_vectors1)\n",
    "\n",
    "    tweet_vectors2 = get_embeddings_from_dataframe(val_df[limit:], column_name='Tweet', batch_size=4096)\n",
    "    tweet_df2 = pd.DataFrame(tweet_vectors2)\n",
    "\n",
    "    period_features = pd.concat([tweet_df1, tweet_df2], axis=1)\n",
    "    ###\n",
    "\n",
    "    period_features = period_features.drop(columns=['Timestamp', 'Tweet'])\n",
    "    period_features = period_features.groupby(['MatchID', 'PeriodID', 'ID']).mean().reset_index()\n",
    "    X_pred = period_features.drop(columns=['MatchID', 'PeriodID', 'ID']).values\n",
    "\n",
    "    preds = clf.predict(X_pred)\n",
    "    period_features['EventType'] = preds\n",
    "    predictions.append(period_features[['ID', 'EventType']])\n",
    "\n",
    "pred_df = pd.concat(predictions)\n",
    "pred_df.to_csv('predictions_rf_all.csv', index=False)\n",
    "\n",
    "\n",
    "print(\"KAGGLE : OK\")\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
