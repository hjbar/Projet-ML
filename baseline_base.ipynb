{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import gensim.downloader as api\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTILS FUNCTIONS :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the average word vector for a tweet\n",
    "def get_avg_embedding(tweet, model, vector_size=200):\n",
    "    words = tweet.split()  # Tokenize by whitespace\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    if not word_vectors:  # If no words in the tweet are in the vocabulary, return a zero vector\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "\n",
    "# Basic preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Tokenization\n",
    "    words = text.split()\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCCESS PART 0 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPROCESS PART 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hbar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/hbar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPROCESS PART 0 : OK\n"
     ]
    }
   ],
   "source": [
    "print(\"PREPROCESS PART 0...\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "\n",
    "os.makedirs(\"tmp/\", exist_ok = True)\n",
    "\n",
    "\n",
    "# Download some NLP models for processing, optional\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load GloVe model with Gensim's API\n",
    "embeddings_model = api.load(\"glove-twitter-200\")  # 200-dimensional GloVe embeddings\n",
    "\n",
    "\n",
    "print(\"PREPROCESS PART 0 : OK\")\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESS PART 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PREPROCESS PART 1...\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "\n",
    "if not os.path.isfile(\"tmp/processing1.csv\"):\n",
    "    # Read all training files and concatenate them into one dataframe\n",
    "    li = []\n",
    "    for filename in os.listdir(\"train_tweets\"):\n",
    "        df = pd.read_csv(\"train_tweets/\" + filename)\n",
    "        li.append(df)\n",
    "    df = pd.concat(li, ignore_index=True)\n",
    "\n",
    "    # Apply preprocessing to each tweet\n",
    "    df['Tweet'] = df['Tweet'].apply(preprocess_text)\n",
    "\n",
    "    df.to_csv(\"tmp/processing1.csv\", index=False, encoding=\"utf-8\")\n",
    "else:\n",
    "    df = pd.read_csv(\"tmp/processing1.csv\")\n",
    "\n",
    "\n",
    "print(\"PREPROCESS PART 1 : OK\")\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESS PART 2 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PREPROCESS PART 2...\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "\n",
    "if not os.path.isfile(\"tmp/X.npy\") and not os.path.isfile(\"tmp/y.npy\"):\n",
    "    # Apply preprocessing to each tweet and obtain vectors\n",
    "    vector_size = 200  # Adjust based on the chosen GloVe model\n",
    "    tweet_vectors = np.vstack([get_avg_embedding(tweet, embeddings_model, vector_size) for tweet in df['Tweet']])\n",
    "    tweet_df = pd.DataFrame(tweet_vectors)\n",
    "\n",
    "    # Attach the vectors into the original dataframe\n",
    "    period_features = pd.concat([df, tweet_df], axis=1)\n",
    "    # Drop the columns that are not useful anymore\n",
    "    period_features = period_features.drop(columns=['Timestamp', 'Tweet'])\n",
    "    # Group the tweets into their corresponding periods. This way we generate an average embedding vector for each period\n",
    "    period_features = period_features.groupby(['MatchID', 'PeriodID', 'ID']).mean().reset_index()\n",
    "\n",
    "    # We drop the non-numerical features and keep the embeddings values for each period\n",
    "    X = period_features.drop(columns=['EventType', 'MatchID', 'PeriodID', 'ID']).values\n",
    "    # We extract the labels of our training samples\n",
    "    y = period_features['EventType'].values\n",
    "\n",
    "    np.save(\"tmp/X.npy\", X)\n",
    "    np.save(\"tmp/y.npy\", y)\n",
    "else:\n",
    "    X = np.load(\"tmp/X.npy\")\n",
    "    y = np.load(\"tmp/y.npy\")\n",
    "\n",
    "\n",
    "print(\"PREPROCESS PART 2 : OK\")\n",
    "sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating on a test set :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split our data into a training and test set that we can use to train our classifier without fine-tuning into the\n",
    "# validation set and without submitting too many times into Kaggle\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Test set: \", accuracy_score(y_test, y_pred))\n",
    "\n",
    "clf = RandomForestClassifier(random_state=42, n_estimators=170, max_depth=10, max_features='sqrt')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Test set: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = XGBClassifier(random_state=42, n_estimators=100, eval_metric=\"logloss\")\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Test set: \", accuracy_score(y_test, y_pred))\n",
    "\n",
    "clf = XGBClassifier(random_state=42, n_estimators=170, learning_rate=0.1, max_depth=6, subsample=1, eval_metric=\"logloss\", booster=\"gbtree\")\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Test set: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Evaluation on a test set :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèles de base\n",
    "rf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "xgb = XGBClassifier(random_state=42, n_estimators=100, eval_metric=\"logloss\")\n",
    "base_models = [ ('rf', rf), ('xgb', xgb) ]\n",
    "\n",
    "# Méta-modèle\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Stacking\n",
    "stack = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=3)\n",
    "stack.fit(X_train, y_train)\n",
    "\n",
    "# Évaluation\n",
    "stack_pred = stack.predict(X_test)\n",
    "print(\"Accuracy (Stacking):\", accuracy_score(y_test, stack_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèles de base\n",
    "rf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "xgb = XGBClassifier(random_state=42, n_estimators=100, eval_metric=\"logloss\")\n",
    "lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "svc =  SVC(kernel=\"rbf\", random_state=42)\n",
    "base_models = [ ('rf', rf), ('xgb', xgb), ('lr', lr), ('svc', svc) ]\n",
    "\n",
    "# Méta-modèle\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Stacking\n",
    "stack = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=3)\n",
    "stack.fit(X_train, y_train)\n",
    "\n",
    "# Évaluation\n",
    "stack_pred = stack.predict(X_test)\n",
    "print(\"Accuracy (Stacking):\", accuracy_score(y_test, stack_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèles de base\n",
    "rf =  RandomForestClassifier(random_state=42, n_estimators=170, max_depth=10, max_features='sqrt')\n",
    "xgb = XGBClassifier(random_state=42, n_estimators=170, learning_rate=0.1, max_depth=6, subsample=1, eval_metric=\"logloss\", booster=\"gbtree\")\n",
    "base_models = [ ('rf', rf), ('xgb', xgb) ]\n",
    "\n",
    "# Méta-modèle\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Stacking\n",
    "stack = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=3)\n",
    "stack.fit(X_train, y_train)\n",
    "\n",
    "# Évaluation\n",
    "stack_pred = stack.predict(X_test)\n",
    "print(\"Accuracy (Stacking):\", accuracy_score(y_test, stack_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèles de base\n",
    "rf =  RandomForestClassifier(random_state=42, n_estimators=170, max_depth=10, max_features='sqrt')\n",
    "xgb = XGBClassifier(random_state=42, n_estimators=170, learning_rate=0.1, max_depth=6, subsample=1, eval_metric=\"logloss\", booster=\"gbtree\")\n",
    "lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "svc =  SVC(kernel=\"rbf\", random_state=42)\n",
    "base_models = [ ('rf', rf), ('xgb', xgb), ('lr', lr), ('svc', svc) ]\n",
    "\n",
    "# Méta-modèle\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Stacking\n",
    "stack = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=3)\n",
    "stack.fit(X_train, y_train)\n",
    "\n",
    "# Évaluation\n",
    "stack_pred = stack.predict(X_test)\n",
    "print(\"Accuracy (Stacking):\", accuracy_score(y_test, stack_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the best cv value :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèles de base\n",
    "rf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "xgb = XGBClassifier(random_state=42, n_estimators=100, eval_metric=\"logloss\")\n",
    "base_models = [ ('rf', rf), ('xgb', xgb) ]\n",
    "    \n",
    "# Méta-modèle\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Find best cv\n",
    "max_score = 0\n",
    "max_cv = 0\n",
    "\n",
    "for i in range(2, 21):\n",
    "    # Stacking\n",
    "    stack = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=i)\n",
    "    stack.fit(X_train, y_train)\n",
    "    \n",
    "    # Évaluation\n",
    "    stack_pred = stack.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, stack_pred)\n",
    "\n",
    "    print(accuracy)\n",
    "\n",
    "    # Udapte ?\n",
    "    if max_score < accuracy:\n",
    "        max_score = accuracy\n",
    "        max_cv = i\n",
    "\n",
    "# Print result\n",
    "print(\"Accuracy (Stacking):\", max_score, \"with cv =\", max_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèles de base\n",
    "rf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "xgb = XGBClassifier(random_state=42, n_estimators=100, eval_metric=\"logloss\")\n",
    "lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "svc =  SVC(kernel=\"rbf\", random_state=42)\n",
    "base_models = [ ('rf', rf), ('xgb', xgb), ('lr', lr), ('svc', svc) ]\n",
    "    \n",
    "# Méta-modèle\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Find best cv\n",
    "max_score = 0\n",
    "max_cv = 0\n",
    "\n",
    "for i in range(2, 21):\n",
    "    # Stacking\n",
    "    stack = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=i)\n",
    "    stack.fit(X_train, y_train)\n",
    "    \n",
    "    # Évaluation\n",
    "    stack_pred = stack.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, stack_pred)\n",
    "\n",
    "    print(accuracy)\n",
    "\n",
    "    # Udapte ?\n",
    "    if max_score < accuracy:\n",
    "        max_score = accuracy\n",
    "        max_cv = i\n",
    "\n",
    "# Print result\n",
    "print(\"Accuracy (Stacking):\", max_score, \"with cv =\", max_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèles de base\n",
    "rf =  RandomForestClassifier(random_state=42, n_estimators=170, max_depth=10, max_features='sqrt')\n",
    "xgb = XGBClassifier(random_state=42, n_estimators=170, learning_rate=0.1, max_depth=6, subsample=1, eval_metric=\"logloss\", booster=\"gbtree\")\n",
    "base_models = [ ('rf', rf), ('xgb', xgb) ]\n",
    "    \n",
    "# Méta-modèle\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Find best cv\n",
    "max_score = 0\n",
    "max_cv = 0\n",
    "\n",
    "for i in range(2, 21):\n",
    "    # Stacking\n",
    "    stack = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=i)\n",
    "    stack.fit(X_train, y_train)\n",
    "    \n",
    "    # Évaluation\n",
    "    stack_pred = stack.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, stack_pred)\n",
    "\n",
    "    print(accuracy)\n",
    "\n",
    "    # Udapte ?\n",
    "    if max_score < accuracy:\n",
    "        max_score = accuracy\n",
    "        max_cv = i\n",
    "\n",
    "# Print result\n",
    "print(\"Accuracy (Stacking):\", max_score, \"with cv =\", max_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèles de base\n",
    "rf =  RandomForestClassifier(random_state=42, n_estimators=170, max_depth=10, max_features='sqrt')\n",
    "xgb = XGBClassifier(random_state=42, n_estimators=170, learning_rate=0.1, max_depth=6, subsample=1, eval_metric=\"logloss\", booster=\"gbtree\")\n",
    "lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "svc =  SVC(kernel=\"rbf\", random_state=42)\n",
    "base_models = [ ('rf', rf), ('xgb', xgb), ('lr', lr), ('svc', svc) ]\n",
    "    \n",
    "# Méta-modèle\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Find best cv\n",
    "max_score = 0\n",
    "max_cv = 0\n",
    "\n",
    "for i in range(2, 21):\n",
    "    # Stacking\n",
    "    stack = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=i)\n",
    "    stack.fit(X_train, y_train)\n",
    "    \n",
    "    # Évaluation\n",
    "    stack_pred = stack.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, stack_pred)\n",
    "\n",
    "    print(accuracy)\n",
    "\n",
    "    # Udapte ?\n",
    "    if max_score < accuracy:\n",
    "        max_score = accuracy\n",
    "        max_cv = i\n",
    "\n",
    "# Print result\n",
    "print(\"Accuracy (Stacking):\", max_score, \"with cv =\", max_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Kaggle submission :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"KAGGLE...\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "\n",
    "# This time we train our classifier on the full dataset that it is available to us.\n",
    "\n",
    "clf = XGBClassifier(random_state=42, n_estimators=170, learning_rate=0.1, max_depth=6, subsample=1, eval_metric=\"logloss\", booster=\"gbtree\")\n",
    "clf.fit(X, y)\n",
    "predictions = []\n",
    "\n",
    "\n",
    "# We read each file separately, we preprocess the tweets and then use the classifier to predict the labels.\n",
    "# Finally, we concatenate all predictions into a list that will eventually be concatenated and exported\n",
    "# to be submitted on Kaggle.\n",
    "for fname in os.listdir(\"eval_tweets\"):\n",
    "    val_df = pd.read_csv(\"eval_tweets/\" + fname)\n",
    "    val_df['Tweet'] = val_df['Tweet'].apply(preprocess_text)\n",
    "\n",
    "    tweet_vectors = np.vstack([get_avg_embedding(tweet, embeddings_model, vector_size) for tweet in val_df['Tweet']])\n",
    "    tweet_df = pd.DataFrame(tweet_vectors)\n",
    "\n",
    "    period_features = pd.concat([val_df, tweet_df], axis=1)\n",
    "    period_features = period_features.drop(columns=['Timestamp', 'Tweet'])\n",
    "    period_features = period_features.groupby(['MatchID', 'PeriodID', 'ID']).mean().reset_index()\n",
    "    \n",
    "    X_pred = period_features.drop(columns=['MatchID', 'PeriodID', 'ID']).values\n",
    "    preds = clf.predict(X_pred)\n",
    "    \n",
    "    period_features['EventType'] = preds\n",
    "    predictions.append(period_features[['ID', 'EventType']])\n",
    "\n",
    "pred_df = pd.concat(predictions)\n",
    "pred_df.to_csv('predictions.csv', index=False)\n",
    "\n",
    "\n",
    "print(\"KAGGLE : OK\")\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
